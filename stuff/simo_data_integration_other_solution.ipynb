{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c7ab8",
   "metadata": {},
   "source": [
    "### Parsing prices data from web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51957c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the html page\n",
    "import requests\n",
    "\n",
    "URL_base = \"https://osservaprezzi.mise.gov.it/prezzi/livelli/beni-e-servizi-di-largo-consumo/archivio-rilevazioni-beni-e-servizi-di-largo-consumo?ANNO=2025&MESE=5&f%5BPROVINCIA%5D=Bologna&f%5BTIPO_RECORD_MISE%5D=\"\n",
    "URL_alimenti = \"altri_alim&submit=Applica\"\n",
    "URL_ortofrutta = \"orto&submit=Applica\"\n",
    "URL_ittici = \"ittici&submit=Applica\"\n",
    "page_alimenti = requests.get(URL_base+URL_alimenti)\n",
    "page_ortofrutta = requests.get(URL_base+URL_ortofrutta)\n",
    "page_ittici = requests.get(URL_base+URL_ittici)\n",
    "\n",
    "print(page_alimenti.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the page with BeautifulSoup\n",
    "soup_al = BeautifulSoup(page_alimenti.content, \"html.parser\")\n",
    "soup_ort = BeautifulSoup(page_ortofrutta.content, \"html.parser\")\n",
    "soup_itt = BeautifulSoup(page_ittici.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96eaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the table with id \"id-table-results\"\n",
    "results_alimenti = soup_al.find(id=\"id-table-results\")\n",
    "results_orto = soup_ort.find(id=\"id-table-results\")\n",
    "results_ittici = soup_itt.find(id=\"id-table-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the table into a pandas dataframe\n",
    "df_prices_alimenti = pd.read_html(StringIO(str(results_alimenti)))[0]\n",
    "df_prices_orto = pd.read_html(StringIO(str(results_orto)))[0]\n",
    "df_prices_ittici = pd.read_html(StringIO(str(results_ittici)))[0]\n",
    "\n",
    "prices_data = pd.concat([df_prices_alimenti, df_prices_orto], ignore_index=True)\n",
    "prices_data = pd.concat([prices_data, df_prices_ittici], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPARQL endpoint and model\n",
    "sparql = SPARQLWrapper(\"http://agrovoc.fao.org/sparql\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "CACHE_PATH = \"agro_cache.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4343c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache():\n",
    "    if not os.path.exists(CACHE_PATH):\n",
    "        return {}\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cache = json.load(f)\n",
    "        return cache\n",
    "\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f, indent=2)\n",
    "\n",
    "def clean_cache():\n",
    "    if not os.path.exists(CACHE_PATH):\n",
    "        return\n",
    "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cache = json.load(f)\n",
    "\n",
    "    cleaned_cache = {\n",
    "        key: value\n",
    "        for key, value in cache.items()\n",
    "        if value.get(\"label\") is not None\n",
    "    }\n",
    "\n",
    "    with open(CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_cache, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d87504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_best_match(value, candidates):\n",
    "    input_embedding = model.encode(value, convert_to_tensor=True)\n",
    "    candidate_texts = []\n",
    "    for c in candidates:\n",
    "        candidate_texts.append(c[\"label\"])\n",
    "    candidate_embeddings = model.encode(candidate_texts, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(input_embedding, candidate_embeddings)[0]\n",
    "    best_idx = scores.argmax().item()\n",
    "    return candidates[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agrovoc(value, cache):\n",
    "\n",
    "    norm_value = value.lower().replace(\",\", \"\")\n",
    "\n",
    "    if norm_value in cache:\n",
    "        print(f\"Cache hit: '{value}' found in cache\")\n",
    "        return cache[norm_value]\n",
    "    \n",
    "    for entry in cache.values():\n",
    "        if \"altLabels\" in entry and value in entry[\"altLabels\"]:\n",
    "            print(f\"Cache hit: '{value}' found in cache\")\n",
    "            return entry\n",
    "    \n",
    "    \n",
    "    print(f\"Cache miss: '{value}' not in cache, querying AGROVOC endpoint\")\n",
    "    # Run SPARQL query\n",
    "    query = f\"\"\"\n",
    "        PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "        SELECT ?concept ?label (GROUP_CONCAT(?altLabel; separator=\" | \") AS ?altLabels) WHERE {{\n",
    "        ?concept skos:prefLabel ?label .\n",
    "        OPTIONAL {{\n",
    "            ?concept skos:altLabel ?altLabel .\n",
    "            FILTER(LANG(?altLabel) = \"en\")\n",
    "        }}\n",
    "\n",
    "        FILTER(LANG(?label) = \"en\")\n",
    "\n",
    "        FILTER(\n",
    "            CONTAINS(LCASE(?label), \"{norm_value}\") ||\n",
    "            (BOUND(?altLabel) && CONTAINS(LCASE(?altLabel), \"{norm_value}\"))\n",
    "        )\n",
    "        }}\n",
    "        GROUP BY ?concept ?label\n",
    "\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    candidates = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        label = result[\"label\"][\"value\"]\n",
    "        uri = result[\"concept\"][\"value\"]\n",
    "        altlabels_str = result[\"altLabels\"][\"value\"] if \"altLabels\" in result else \"\"\n",
    "        altlabels = [al.strip().lower() for al in altlabels_str.split(\"|\") if al.strip()]\n",
    "        candidate = {\"label\": label, \"uri\": uri, \"altLabels\": altlabels}\n",
    "        candidates.append(candidate)\n",
    "        \n",
    "    if not candidates:\n",
    "        match = {\"label\": None, \"uri\": None, \"altLabels\": []}\n",
    "    else:\n",
    "        best = semantic_best_match(value, candidates)\n",
    "        match = best\n",
    "    # Cache the result\n",
    "    cache[norm_value] = match\n",
    "    save_cache(cache)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1be1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_agrovoc(df, column_name):\n",
    "    cache = load_cache()\n",
    "    labels = []\n",
    "    uris = []\n",
    "    for val in df[column_name]:\n",
    "        match = query_agrovoc(val, cache)\n",
    "        labels.append(match[\"label\"])\n",
    "        uris.append(match[\"uri\"])\n",
    "    df[\"AGROVOC_label\"] = labels\n",
    "    df[\"AGROVOC_uri\"] = uris\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the food consumption data\n",
    "df_consumption = pd.read_csv(\"data/chronic_consumption_gday_allsubjects.csv\", encoding=\"utf-16\")\n",
    "\n",
    "# Load the SuEatableLife dataset\n",
    "df_wf = pd.read_excel(\"data/sueatablelife_dataset.xlsx\", sheet_name=\"SEL WF for users\")\n",
    "df_cf = pd.read_excel(\"data/sueatablelife_dataset.xlsx\", sheet_name=\"SEL CF for users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove water from the consumption data\n",
    "df_consumption = df_consumption[(df_consumption[\"Exposure hierarchy (L7)\"] != \"Natural mineral water\") &\n",
    "    (df_consumption[\"Exposure hierarchy (L7)\"] != \"Tap water\") &\n",
    "    (df_consumption[\"Exposure hierarchy (L7)\"] != \"Filtered tap water\")]\n",
    "\n",
    "# Group by \"Exposure hierarchy (L5)\" and sum the \"Mean\" values\n",
    "top_n = df_consumption.groupby(\"Exposure hierarchy (L5)\", as_index=False)[\"Mean\"].sum().sort_values('Mean', ascending=False).head(15)\n",
    "\n",
    "top_n[[\"Exposure hierarchy (L5)\", \"Mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge coffee rows and milk rows\n",
    "rows_to_merge = top_n[top_n[\"Exposure hierarchy (L5)\"].isin([\"Coffee (average strength) beverage\", \"Coffee espresso (beverage)\"])]\n",
    "merged_row = rows_to_merge.sum(numeric_only=True)\n",
    "merged_row[\"Exposure hierarchy (L5)\"] = \"Coffee\"\n",
    "\n",
    "# Drop the original rows and append the merged row\n",
    "top_n = top_n[~top_n[\"Exposure hierarchy (L5)\"].isin([\"Coffee (average strength) beverage\", \"Coffee espresso (beverage)\"])]\n",
    "top_n = pd.concat([top_n, pd.DataFrame([merged_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d878cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort again by \"Mean\" values to include the merged rows\n",
    "top_n = top_n.sort_values(\"Mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58299df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the top_n DataFrame with AGROVOC labels and URIs\n",
    "top_n_enriched = enrich_with_agrovoc(top_n, \"Exposure hierarchy (L5)\")\n",
    "\n",
    "top_n_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for pre-processing\n",
    "# Function to singularize English words based on common pluralization rules.\n",
    "# List of (pattern, replacement) in priority order:\n",
    "PLURAL_RULES = [\n",
    "    # -kies → -kie  (e.g. “cookies” → “cookie”)\n",
    "    (r'(?i)([a-z]+)kies$', r'\\1kie'),\n",
    "    # -ies → -y      (e.g. “berries” → “berry” but not “cookies”)\n",
    "    (r'(?i)([a-z]+)ies$', r'\\1y'),\n",
    "    # -ves → -f      (e.g. “wolves” → “wolf”)\n",
    "    (r'(?i)([a-z]+)ves$', r'\\1f'),\n",
    "    # -oes → -oe     (e.g. “heroes” → “heroe”—rare, you may want 'o')\n",
    "    (r'(?i)([a-z]+)oes$', r'\\1oe'),\n",
    "    # -ses → -s      (e.g. “dresses” → “dress”)\n",
    "    (r'(?i)([a-z]+)ses$', r'\\1s'),\n",
    "    # -xes → -x      (e.g. “boxes” → “box”)\n",
    "    (r'(?i)([a-z]+)xes$', r'\\1x'),\n",
    "    # -ches/-shes → -ch/-sh  (e.g. “churches” → “church”)\n",
    "    (r'(?i)([a-z]+(?:ch|sh))es$', r'\\1'),\n",
    "    # -s → ''        (catch‑all; e.g. “cars” → “car”)\n",
    "    (r'(?i)([a-z]+)s$', r'\\1'),\n",
    "]\n",
    "\n",
    "def singularize(word: str) -> str:\n",
    "    \"\"\"Apply common English plural→singular regex rules.\"\"\"\n",
    "    for pattern, repl in PLURAL_RULES:\n",
    "        if re.search(pattern, word):\n",
    "            return re.sub(pattern, repl, word)\n",
    "    return word  # no rule matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize labels\n",
    "def normalize_label(s, language='en'):\n",
    "    s = s.lower().strip().replace('-', ' ').replace('_', ' ')\n",
    "     # Remove numbers and letters in brackets, asteriks, punctuation\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)  # Remove text in brackets\n",
    "    s = re.sub(r'\\d+', '', s)  # Remove numbers\n",
    "    s = re.sub(r'\\*', '', s)  # Remove asterisks\n",
    "    s = re.sub(r'[^a-z0-9 ]+', ' ', s)       # drop punctuation\n",
    "    s = re.sub(r'\\b(semi[- ]skimmed|organic|low[- ]fat|mixed)\\b', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)               # collapse whitespace\n",
    "    s = re.sub(r\"(fresh|caffeinic|common)\", \"\", s) # remove common useless attributes\n",
    "    s = re.sub(r\"wheat bread\", \"bread\", s)\n",
    "    s = s.strip()  # Remove leading and trailing whitespace\n",
    "    s = re.sub(r\"meat\", \"\", s) # remove meat to improve the results of fuzzy matching for different animal meat\n",
    "    s = re.sub(r\"\\sor\\s\", \" \", s)\n",
    "    # handling plurals\n",
    "    if language == 'en':\n",
    "        s = singularize(s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the AGROVOC labels of the first dataframe\n",
    "top_n['clean_label'] = top_n[\"AGROVOC_label\"].apply(normalize_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b54d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the labels in the other dataframes\n",
    "df_cf['clean_label'] = df_cf[\"Food commodity ITEM\"].apply(normalize_label)\n",
    "df_wf['clean_label'] = df_wf[\"Food commodity ITEM\"].apply(normalize_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b91d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a mapping from a dataframe column to the closest values in another dataframe column using fuzzy matching\n",
    "def create_fuzzy_mapping(source_df, target_df, source_col, target_col, score_cut):\n",
    "    matches = {}\n",
    "    for value in source_df[source_col]:\n",
    "        result = process.extractOne(value, target_df[target_col], score_cutoff=score_cut)\n",
    "        if result is not None:\n",
    "            match, score, _ = result\n",
    "            matches[value] = match\n",
    "        # Keep unmatched values as None\n",
    "        else:\n",
    "            matches[value] = None\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = create_fuzzy_mapping(top_n, df_cf, 'clean_label', 'clean_label', 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a87df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the matched food to the top_n DataFrame, but keep unmatched values as None\n",
    "top_n['matched_food'] = top_n['clean_label'].map(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the top_n DataFrame with the cF DataFrame on the matched food, keeping unmatched values as None\n",
    "merged_df = top_n.merge(df_cf, left_on='matched_food', right_on='clean_label', how=\"left\", suffixes=('_cons', '_cf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea23684",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbef3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Search for coffee rows in the cF DataFrame\n",
    "\n",
    "df_cf[df_cf[\"clean_label\"].str.contains(\"coffe\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct coffee row in the cF DataFrame\n",
    "single_coffee_row = df_cf[df_cf[\"clean_label\"] == \"coffee drip filtered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to substitute part of the rows in a DataFrame\n",
    "# This function substitutes the last n columns of rows in df1 where the specified column matches value\n",
    "def substitute_part_rows(df1, df2, value1, column_name, n_columns, df3=None, df4=None, value2=None, value3=None):\n",
    "    for idx, row in df1.iterrows():\n",
    "        if pd.isna(row[column_name]):\n",
    "            continue\n",
    "        if value1 in row[column_name].lower():\n",
    "            df1.iloc[idx, -n_columns:] = df2.iloc[0,:n_columns].values\n",
    "        if df3 is not None:\n",
    "            if value2 in row[column_name].lower():\n",
    "                df1.iloc[idx, -n_columns:] = df3.iloc[0,:n_columns].values\n",
    "        if df4 is not None:\n",
    "            if value3 in row[column_name].lower():\n",
    "                df1.iloc[idx, -n_columns:] = df4.iloc[0,:n_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Substitute the values in the merged DataFrame for the coffee row\n",
    "for idx, row in merged_df.iterrows():\n",
    "    if row[\"Food commodity ITEM\"] == \"COFFEE GROUND\":\n",
    "        merged_df.iloc[idx, -10:] = single_coffee_row.iloc[0,:10].values\n",
    "        break \"\"\"\n",
    "\n",
    "# Use the function to substitute the values in the merged DataFrame for the coffee row\n",
    "substitute_part_rows(merged_df, single_coffee_row, \"COFFEE\", \"Food commodity ITEM\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1852aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=[\"Exposure hierarchy (L5)\", \"Uncertainty    low (L) high (H)\", \"matched_food\", \"FOOD COMMODITY GROUP\", \"Food commodity sub-TYPOLOGY\", \"Carbon Footprint g CO2eq/g o cc of food sub-TYPOLOGY\", \"clean_label_cf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = create_fuzzy_mapping(merged_df, df_wf, 'clean_label_cons', 'clean_label', 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f67a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['matched_food_wf'] = merged_df['clean_label_cons'].map(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_df = merged_df.merge(df_wf, left_on='matched_food_wf', right_on='clean_label', how=\"left\", suffixes=('_cons', '_wf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dried apples (wrong match)\n",
    "new_merged_df = new_merged_df[~(new_merged_df[\"Food commodity ITEM_wf\"] == \"APPLES\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Choose the correct coffee\n",
    "df_wf[df_wf[\"clean_label\"].str.contains(\"coffee\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_coffee_row = df_wf[df_wf[\"clean_label\"] == \"coffee standard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_part_rows(new_merged_df, single_coffee_row, \"COFFEE ROASTED\", \"Food commodity ITEM_wf\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the prices data\n",
    "prices_data['clean_label'] = prices_data[\"Descrizione Prodotto\"].apply(normalize_label, language='it')\n",
    "\n",
    "prices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b86e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def find_semantic_matches(source_df, candidate_df, source_column, candidate_column, model_name='paraphrase-multilingual-mpnet-base-v2'):\n",
    "    \"\"\"\n",
    "    Finds the best semantic match for each item in a source DataFrame column from a candidate DataFrame column.\n",
    "\n",
    "    Args:\n",
    "        source_df (pd.DataFrame): The DataFrame containing the items to be matched.\n",
    "        candidate_df (pd.DataFrame): The DataFrame containing the pool of candidate items.\n",
    "        source_column (str): The name of the column in source_df to match from.\n",
    "        candidate_column (str): The name of the column in candidate_df to match against.\n",
    "        model_name (str): The name of the sentence-transformer model to use.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The source DataFrame with a new 'best_match' column.\n",
    "    \"\"\"\n",
    "    # 1. Load the pre-trained multilingual model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # 2. Get the lists of strings to compare\n",
    "    source_items = source_df[source_column]\n",
    "    candidate_items = candidate_df[candidate_column]\n",
    "\n",
    "    # 3. Encode the candidate strings into embeddings (done only once for efficiency)\n",
    "    print(\"Encoding candidate embeddings...\")\n",
    "    candidate_embeddings = model.encode(candidate_items.tolist(), convert_to_tensor=True)\n",
    "    print(\"Encoding complete.\")\n",
    "\n",
    "    # 4. Iterate through the source column to find the best match for each item\n",
    "    best_matches = []\n",
    "    print(f\"Finding best matches for {len(source_items)} source items...\")\n",
    "    for item in source_items:\n",
    "        # Encode the source item\n",
    "        item_embedding = model.encode(item, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity between the item and all candidates\n",
    "        cosine_scores = util.cos_sim(item_embedding, candidate_embeddings)\n",
    "        \n",
    "        # Find the index of the highest score\n",
    "        best_match_index = torch.argmax(cosine_scores).item()\n",
    "        \n",
    "        # Get the best matching string from the candidate list\n",
    "        best_match_string = candidate_items.iloc[best_match_index]\n",
    "        \n",
    "        best_matches.append(best_match_string)\n",
    "    \n",
    "    print(\"Matching complete.\")\n",
    "    # 5. Add the results to the source DataFrame\n",
    "    source_df['best_match'] = best_matches\n",
    "    \n",
    "    return source_df\n",
    "\n",
    "# --- Example of how to use the function with your actual data ---\n",
    "# Find the best price description for each food item in your merged dataframe\n",
    "final_df = find_semantic_matches(\n",
    "    source_df=new_merged_df,\n",
    "    candidate_df=prices_data,\n",
    "    source_column='clean_label_cons',\n",
    "    candidate_column='clean_label'\n",
    ")\n",
    "\n",
    "# Display the key columns to check the results\n",
    "final_df[['clean_label_cons', 'best_match']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd83843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the final DataFrame with prices data\n",
    "final_prices_df = final_df.merge(prices_data, left_on='best_match', right_on='clean_label', how=\"left\", suffixes=('_con', '_price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40903222",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee869982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Choose the correct food items\n",
    "# For beef, pasta, and tomato, we will select the rows that match our criteria\n",
    "\n",
    "beef_price_row = prices_data[prices_data[\"Descrizione Prodotto\"].str.contains(\"bovino adulto\", case=False, na=False)]\n",
    "pasta_price_row = prices_data[prices_data[\"Descrizione Prodotto\"].str.contains(\"pasta\", case=False, na=False)]\n",
    "tomato_price_row = prices_data[prices_data[\"Descrizione Prodotto\"].str.contains(\"pomodori\", case=False, na=False)]\n",
    "\n",
    "tomato_price_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf71e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the values in the final_prices_df\n",
    "substitute_part_rows(final_prices_df, beef_price_row, \"carne\", \"Descrizione Prodotto\", 6, df3=pasta_price_row, value2=\"pasta\", df4=tomato_price_row, value3=\"pomodoro\")\n",
    "\n",
    "\"\"\" for idx, row in final_prices_df.iterrows():\n",
    "    if \"carne\" in row[\"Descrizione Prodotto\"].lower():\n",
    "        final_prices_df.iloc[idx, -6:] = beef_price_row.iloc[0,:6].values\n",
    "    elif \"pasta\" in row[\"Descrizione Prodotto\"].lower():\n",
    "        final_prices_df.iloc[idx, -6:] = pasta_price_row.iloc[0,:6].values\n",
    "    elif \"pomodoro\" in row[\"Descrizione Prodotto\"].lower():\n",
    "        final_prices_df.iloc[idx, -6:] = tomato_price_row.iloc[0,:6].values \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ced9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data for peaches\n",
    "# Put NaN for last seven columns of the row with peaches\n",
    "\n",
    "peach_row = final_prices_df[final_prices_df[\"clean_label_con\"].str.contains(\"peach\", case=False, na=False)]\n",
    "\n",
    "for idx, row in peach_row.iterrows():\n",
    "    final_prices_df.iloc[idx, -7:] = [np.nan] * 7  # Set the last seven columns to NaN\n",
    "\n",
    "final_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260004ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function to determine the final CF and WF values\n",
    "def get_final_values(row):\n",
    "    cf_value = np.nan\n",
    "    wf_value = np.nan\n",
    "    \n",
    "    if row[\"Suggested CF value\"] in [\"OK item\", \"Item matching typology\"]:\n",
    "        cf_value = row[\"Carbon Footprint kg CO2eq/kg or l of food ITEM\"]\n",
    "    elif row[\"Suggested CF value\"] == \"better subtypology\":\n",
    "        cf_value = row[\"Carbon Footprint g CO2eq/g o cc of food sub-TYPOLOGY\"]\n",
    "    elif row[\"Suggested CF value\"] == \"better typology\":\n",
    "        cf_value = row[\"Carbon Footprint g CO2eq/g o cc of food TYPOLOGY\"]\n",
    "    \n",
    "    if row[\"Suggested WF value\"] in [\"ok item\", \"item matching typology\"]:\n",
    "        wf_value = row[\"Water Footprint liters water/kg o liter of food ITEM\"]\n",
    "    elif row[\"Suggested WF value\"] == \"better subtypology\":\n",
    "        wf_value = row[\"Water Footprint cc water/g o cc of food sub-TYPOLOGY\"]\n",
    "    elif row[\"Suggested WF value\"] == \"better typology\":\n",
    "        wf_value = row[\"Water Footprint cc water/g o cc of food TYPOLOGY\"]\n",
    "    \n",
    "    return pd.Series([cf_value, wf_value])\n",
    "\n",
    "# Apply the helper function to the DataFrame\n",
    "final_prices_df[[\"Final CF value\", \"Final WF value\"]] = final_prices_df.apply(get_final_values, axis=1)\n",
    "\n",
    "# Display the final DataFrame with the new columns\n",
    "final_prices_df[[\"Food commodity ITEM_cons\", \"Final CF value\", \"Food commodity ITEM_wf\", \"Final WF value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd1bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices_df[\"Descrizione Prodotto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the food quantity and unit from the description\n",
    "def extract_food_quantity(description):\n",
    "    # Use regex to find the quantity in the description\n",
    "    if isinstance(description, str):\n",
    "        match = re.search(r\"(\\d+)\\s*([a-zA-Z]{2})\", description, re.IGNORECASE)\n",
    "        if match:\n",
    "            quantity = match.group(1)\n",
    "            unit = match.group(2).lower()\n",
    "            return int(quantity), unit\n",
    "    return None, None  # Return None if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to extract food quantity and unit\n",
    "final_prices_df['food_quantity'], final_prices_df['food_unit'] = zip(*final_prices_df['Descrizione Prodotto'].apply(extract_food_quantity))\n",
    "# Display the final DataFrame with the new columns\n",
    "final_prices_df[[\"Food commodity ITEM_cons\", \"Final CF value\", \"Food commodity ITEM_wf\", \"Final WF value\", \"food_quantity\", \"food_unit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb655ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cost per kg or liter\n",
    "def calculate_cost_per_kg_or_liter(row):\n",
    "    if row['food_unit'] in ['kg', 'l']:\n",
    "        return row['Quotazione Media'] / row['food_quantity']\n",
    "    elif row['food_unit'] in ['gr', 'ml']:\n",
    "        return (row['Quotazione Media'] / row['food_quantity']) * 1000  # Convert g to kg and cc to l\n",
    "    elif row['food_unit'] in [\"cl\"]:\n",
    "        return (row['Quotazione Media'] / row['food_quantity']) * 100\n",
    "    else:\n",
    "        return None  # Return None for unsupported units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to calculate the cost per kg or liter\n",
    "final_prices_df['cost_per_kg_or_l'] = final_prices_df.apply(calculate_cost_per_kg_or_liter, axis=1)\n",
    "\n",
    "# Display the final DataFrame with the new column\n",
    "final_prices_df[[\"Food commodity ITEM_cons\", \"Final CF value\", \"Food commodity ITEM_wf\", \"Final WF value\", \"food_quantity\", \"food_unit\", \"cost_per_kg_or_l\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c851a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant columns\n",
    "final_prices_df = final_prices_df[[\"AGROVOC_label\", \"AGROVOC_uri\", \"FOOD COMMODITY GROUP_cons\", \"Mean\", \"Final CF value\", \"Final WF value\", \"cost_per_kg_or_l\"]]\n",
    "\n",
    "final_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns for clarity\n",
    "final_prices_df.columns = [\"AGROVOC_label\", \"AGROVOC_uri\", \"food_typology\", \"Mean_consumption_italy\", \"Carbon Footprint (g CO2eq/g o cc)\", \"(Water Footprint liters) water/kg o liter\", \"cost_per_kg_or_l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the three tomato rows into one by averaging the values\n",
    "definitive_df = final_prices_df.groupby(\"AGROVOC_label\").agg({\"AGROVOC_uri\": \"first\", \"food_typology\": \"first\", \"Mean_consumption_italy\": \"first\", \"Carbon Footprint (g CO2eq/g o cc)\": \"mean\", \"(Water Footprint liters) water/kg o liter\": \"mean\", \"cost_per_kg_or_l\": \"first\"}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the definitive DataFrame by Mean_consumption_italy\n",
    "definitive_df = definitive_df.sort_values(by=\"Mean_consumption_italy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "definitive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a91990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to a CSV file\n",
    "definitive_df.to_csv(\"site/final_data/italy_food_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6adb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the correct data types to the first three columns\n",
    "definitive_df[\"AGROVOC_label\"] = definitive_df[\"AGROVOC_label\"].astype(str)\n",
    "definitive_df[\"AGROVOC_uri\"] = definitive_df[\"AGROVOC_uri\"].astype(str)\n",
    "definitive_df[\"food_typology\"] = definitive_df[\"food_typology\"].astype(str)\n",
    "\n",
    "print(definitive_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between carbon footprint and price\n",
    "definitive_df_nona = definitive_df.dropna(subset=[\"Mean_consumption_italy\", \"Carbon Footprint (g CO2eq/g o cc)\", \"(Water Footprint liters) water/kg o liter\", \"cost_per_kg_or_l\"])\n",
    "definitive_df_nona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ce586",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitive_df[\"AGROVOC_label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c69325",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = definitive_df_nona.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(numeric_df[\"Carbon Footprint (g CO2eq/g o cc)\"], numeric_df[\"cost_per_kg_or_l\"])\n",
    "plt.title(\"cf vs cost per kg or liter\")\n",
    "plt.xlabel('cf')\n",
    "plt.ylabel('cost per kg or liter')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0a3dd",
   "metadata": {},
   "source": [
    "### Selecting other food for the menu and creating the dataframe with all the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97be7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_consumption = df_consumption.groupby(\"Exposure hierarchy (L5)\", as_index=False)[\"Mean\"].sum().sort_values('Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7113d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select rows based on a list of food items (even if they are not exact matches)\n",
    "def select_rows_by_food_items(df, food_items):\n",
    "    selected_rows = []\n",
    "    for item in food_items:\n",
    "        # Use regex to find rows that contain the food item (case-insensitive)\n",
    "        pattern = re.compile(re.escape(item), re.IGNORECASE)\n",
    "        matching_rows = df[df['Exposure hierarchy (L5)'].str.contains(pattern, na=False)]\n",
    "        selected_rows.append(matching_rows)\n",
    "    return pd.concat(selected_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_food_items = [\"yogurt\", \"ham\", \"tuna\", \"soya drink\", \"peas\", \"pears\", \"rice\", \"onion\", \"carrot\", \"zucch\", \"juice, orange\", \"salad\", \"eggplant\", \"orange\", \"banana\", \"salmon\", \"cod\", \"chocolate\", \"ice cream, milk-based\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f205219",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_food = select_rows_by_food_items(food_consumption, list_food_items)\n",
    "\n",
    "print(rows_food[\"Exposure hierarchy (L5)\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove rows that contain specific substrings in a column\n",
    "def remove_rows_with_substrings(df, column_name, substrings):\n",
    "    for substring in substrings:\n",
    "        pattern = re.compile(re.escape(substring), re.IGNORECASE)\n",
    "        df = df[~df[column_name].str.contains(pattern, na=False)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows = remove_rows_with_substrings(rows_food, \"Exposure hierarchy (L5)\", [\"Broad\", \"Simple\", \"with pods\", \"French\", \"Fried\", \"Boiled\", \"Poached\", \"Common Quail\", \"cake\", \"chocolate sauce\", \"Couverture chocolate\", \"chocolate cake\", \"croissant\", \"chocolate spread\", \"Chocolate coated confectionery\", \"Cooked\", \"Ham, beef\", \"Bechamel\", \"Sandwich\", \"without pods\", \"Rice drink\", \"bread\", \"popped\", \"Liquorice\", \"Starch\", \"flour\", \"Smoked\", \"Canned\", \"Salted\"])\n",
    "\n",
    "# Remove duplicates based on the \"Exposure hierarchy (L5)\" column\n",
    "food_rows = food_rows.drop_duplicates(subset=[\"Exposure hierarchy (L5)\"])\n",
    "# Reset the index of the DataFrame\n",
    "food_rows = food_rows.reset_index(drop=True)\n",
    "\n",
    "food_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the chocolate rows and take the mean of the values\n",
    "chocolate_rows = food_rows[food_rows[\"Exposure hierarchy (L5)\"].str.contains(\"chocolate\", case=False, na=False)]\n",
    "chocolate_rows = chocolate_rows[~chocolate_rows[\"Exposure hierarchy (L5)\"].str.contains(\"biscuit\", case=False, na=False)]\n",
    "\n",
    "# Remove attributes like Bitter, Milk, White, Dark, etc. from the chocolate rows\n",
    "chocolate_rows[\"Exposure hierarchy (L5)\"] = chocolate_rows[\"Exposure hierarchy (L5)\"].str.replace(r'\\b(bitter|milk|white|Filled|Gianduja)\\b', '', regex=True, case=False).str.strip()\n",
    "\n",
    "chocolate_rows = chocolate_rows.groupby(\"Exposure hierarchy (L5)\", as_index=False).mean()\n",
    "\n",
    "chocolate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows containing \"chocolate\" but keep \"Biscuits, chocolate\"\n",
    "food_rows = food_rows[~(food_rows[\"Exposure hierarchy (L5)\"].str.contains(\"chocolate\", case=False, na=False) & \n",
    "                        ~food_rows[\"Exposure hierarchy (L5)\"].str.contains(\"Biscuits, chocolate\", case=False, na=False))]\n",
    "\n",
    "# Append the merged chocolate rows\n",
    "food_rows = pd.concat([food_rows, chocolate_rows], ignore_index=True)\n",
    "\n",
    "food_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73390664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows 3, 6, 10, 13\n",
    "rows_to_remove = [3, 6, 10, 13]\n",
    "food_rows = food_rows.drop(index=rows_to_remove).reset_index(drop=True)\n",
    "\n",
    "food_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65614d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the food_rows DataFrame with AGROVOC labels and URIs\n",
    "food_rows_enriched = enrich_with_agrovoc(food_rows, \"Exposure hierarchy (L5)\")\n",
    "\n",
    "food_rows_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the labels in the food_rows DataFrame\n",
    "food_rows['clean_label'] = food_rows[\"AGROVOC_label\"].apply(normalize_label, language='en')\n",
    "\n",
    "food_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde16ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for matches with the cf DataFrame using semantic matching\n",
    "food_rows_matched = find_semantic_matches(\n",
    "    source_df=food_rows,\n",
    "    candidate_df=df_cf,\n",
    "    source_column='clean_label',\n",
    "    candidate_column='clean_label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47efc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the food_rows DataFrame with the cF DataFrame on the matched food, keeping unmatched values as None\n",
    "food_rows_merged = food_rows_matched.merge(df_cf, left_on='best_match', right_on='clean_label', how=\"left\", suffixes=('_food', '_cf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Choose the correct ham row in the cF DataFrame\n",
    "ham_rows = df_cf[df_cf[\"clean_label\"].str.contains(\"ham\", case=False, na=False)]\n",
    "\n",
    "ham_row = ham_rows.iloc[1].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct biscuit row in the cF DataFrame\n",
    "biscuit_rows = df_cf[df_cf[\"clean_label\"].str.contains(\"biscuit|cookie\", case=False, na=False)]\n",
    "biscuit_row = biscuit_rows.iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct chocolate row in the cF DataFrame\n",
    "chocolate_rows = df_cf[df_cf[\"clean_label\"].str.contains(\"dark chocolate|milk chocolate|white chocolate\", case=False, na=False)]\n",
    "\n",
    "chocolate_row = chocolate_rows.iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18838b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the values in the food_rows_merged DataFrame for the ham, biscuit, and chocolate rows\n",
    "substitute_part_rows(food_rows_merged, ham_row, \"hake\", \"best_match\", 10, df3=biscuit_row, value2=\"crispbread\", df4=chocolate_row, value3=\"chocolate\")\n",
    "\n",
    "food_rows_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates. To choose the best row, we will keep the one without the (F) in the \"Food commodity ITEM\" column\n",
    "def keep_best_rows(df, column_name):\n",
    "    def pick_best(group, column_name=column_name):\n",
    "        # Try to find a row without (F)\n",
    "        non_f = group[~group[column_name].str.contains(r\"\\(F\\)|\\(fresh\\)\", na=False)]\n",
    "        if not non_f.empty:\n",
    "            return non_f.iloc[0]\n",
    "        else:\n",
    "            return group.iloc[0]\n",
    "    return df.groupby(\"best_match\", as_index=False, group_keys=False).apply(pick_best, include_groups = False).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_merged = keep_best_rows(food_rows_merged, \"Food commodity ITEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic matching with the water footprint DataFrame\n",
    "food_rows_wf_matched = find_semantic_matches(\n",
    "    source_df=food_rows_merged,\n",
    "    candidate_df=df_wf,\n",
    "    source_column='clean_label_food',\n",
    "    candidate_column='clean_label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_wf_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c627c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the merged DataFrame with the water footprint DataFrame on the matched food, keeping unmatched values as None\n",
    "food_rows_wf_merged = food_rows_wf_matched.merge(df_wf, left_on='best_match', right_on='clean_label', how=\"left\", suffixes=('_food', '_wf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_wf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78482aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Choose the correct ham row in the wF DataFrame\n",
    "ham_rows = df_wf[df_wf[\"clean_label\"].str.contains(\"ham\", case=False, na=False)]\n",
    "\n",
    "ham_row = ham_rows.iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92116d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct biscuit row in the wF DataFrame\n",
    "biscuit_rows = df_wf[df_wf[\"clean_label\"].str.contains(\"biscuit|cookie\", case=False, na=False)]\n",
    "biscuit_row = biscuit_rows.iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec98331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct ice cream row in the wF DataFrame\n",
    "ice_cream_rows = df_wf[df_wf[\"clean_label\"].str.contains(\"ice cream\", case=False, na=False)]\n",
    "\n",
    "ice_cream_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the correct tuna row in the wF DataFrame\n",
    "tuna_rows = df_wf[df_wf[\"clean_label\"].str.contains(\"tuna\", case=False, na=False)]\n",
    "tuna_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the values in the food_rows_merged DataFrame for the ham, biscuit, and chocolate rows\n",
    "substitute_part_rows(food_rows_wf_merged, ham_row, \"kephir\", \"best_match\", 10, df3=biscuit_row, value2=\"crispbread\")\n",
    "\n",
    "food_rows_wf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_wf_merged = keep_best_rows(food_rows_wf_merged, \"Food commodity ITEM_wf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d254c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are no tuna and no ice cream rows in the wF DataFrame, we will assign NaN to the last seven columns of the tuna and ice cream rows in the food_rows_wf_merged DataFrame\n",
    "for idx, row in food_rows_wf_merged.iterrows():\n",
    "    if \"tuna\" in row[\"Food commodity ITEM_food\"].lower():\n",
    "        food_rows_wf_merged.iloc[idx, -10:] = [np.nan] * 10  # Set the last seven columns to NaN\n",
    "    elif \"ice cream\" in row[\"Food commodity ITEM_food\"].lower():\n",
    "        food_rows_wf_merged.iloc[idx, -10:] = [np.nan] * 10  # Set the last seven columns to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_wf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a04b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic matching with prices DataFrame\n",
    "food_rows_prices_matched = find_semantic_matches(\n",
    "    source_df=food_rows_wf_merged,\n",
    "    candidate_df=prices_data,\n",
    "    source_column='clean_label_food',\n",
    "    candidate_column='clean_label'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead99cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_prices_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the food_prices_matched DataFrame with the prices DataFrame on the matched food, keeping unmatched values as None\n",
    "food_rows_prices_merged = food_rows_prices_matched.merge(prices_data, left_on='best_match', right_on='clean_label', how=\"left\", suffixes=('_food1', '_price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_prices_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4458817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually fix the data\n",
    "# Choose the correct ham row in the prices DataFrame\n",
    "ham_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"prosciutto\", case=False, na=False)]\n",
    "ham_row = ham_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct cod row in the prices DataFrame\n",
    "cod_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"merluzzi\", case=False, na=False)]\n",
    "cod_row = cod_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct orange row in the prices DataFrame\n",
    "orange_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"arance\", case=False, na=False)]\n",
    "orange_row = orange_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct rice row in the prices DataFrame\n",
    "rice_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"riso\", case=False, na=False)]\n",
    "rice_row = rice_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct soy milk row in the prices DataFrame\n",
    "soy_milk_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"vegetali\", case=False, na=False)]\n",
    "soy_milk_row = soy_milk_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct peas row in the prices DataFrame\n",
    "peas_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"piselli\", case=False, na=False)]\n",
    "peas_row = peas_rows.iloc[0].to_frame().T\n",
    "\n",
    "# Choose the correct pear row in the prices DataFrame\n",
    "pear_rows = prices_data[prices_data[\"clean_label\"].str.contains(\"pere\", case=False, na=False)]\n",
    "pear_row = pear_rows.iloc[0].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the values in the food_rows_prices_merged DataFrame for the ham, cod, orange, rice, and soy milk rows\n",
    "substitute_part_rows(food_rows_prices_merged, ham_row, \"hams\", \"AGROVOC_label\", 6, df3=cod_row, value2=\"cod\", df4=orange_row, value3=\"sweet oranges\")\n",
    "substitute_part_rows(food_rows_prices_merged, rice_row, \"rice\", \"AGROVOC_label\", 6, df3=soy_milk_row, value2=\"soy milk\", df4=peas_row, value3=\"peas\")\n",
    "substitute_part_rows(food_rows_prices_merged, pear_row, \"pears\", \"AGROVOC_label\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a72c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_rows_prices_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the helper function to the DataFrame\n",
    "food_rows_prices_merged[[\"Final CF value\", \"Final WF value\"]] = food_rows_prices_merged.apply(get_final_values, axis=1)\n",
    "\n",
    "# Apply the function to extract food quantity and unit\n",
    "food_rows_prices_merged['food_quantity'], food_rows_prices_merged['food_unit'] = zip(*food_rows_prices_merged['Descrizione Prodotto'].apply(extract_food_quantity))\n",
    "\n",
    "# Apply the function to calculate the cost per kg or liter\n",
    "food_rows_prices_merged['cost_per_kg_or_l'] = food_rows_prices_merged.apply(calculate_cost_per_kg_or_liter, axis=1)\n",
    "\n",
    "# Keep only the relevant columns\n",
    "food_rows_prices_merged = food_rows_prices_merged[[\"AGROVOC_label\", \"AGROVOC_uri\", \"FOOD COMMODITY GROUP_food\", \"Mean\", \"Final CF value\", \"Final WF value\", \"cost_per_kg_or_l\"]]\n",
    "\n",
    "# Rename the columns for clarity\n",
    "food_rows_prices_merged.columns = [\"AGROVOC_label\", \"AGROVOC_uri\", \"food_typology\", \"Mean_consumption_italy\", \"Carbon Footprint (g CO2eq/g o cc)\", \"(Water Footprint liters) water/kg o liter\", \"cost_per_kg_or_l\"]\n",
    "\n",
    "# Sorting the definitive DataFrame by Mean_consumption_italy\n",
    "definitive_df_more_food = food_rows_prices_merged.sort_values(by=\"Mean_consumption_italy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "definitive_df_more_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dedb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the two definitive DataFrames\n",
    "final_definitive_df = pd.concat([definitive_df, definitive_df_more_food], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame to a CSV file\n",
    "final_definitive_df.to_csv(\"site/final_data/game_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between carbon footprint and price\n",
    "final_definitive_df_nona = final_definitive_df.dropna(subset=[\"Mean_consumption_italy\", \"Carbon Footprint (g CO2eq/g o cc)\", \"(Water Footprint liters) water/kg o liter\", \"cost_per_kg_or_l\"])\n",
    "final_definitive_df_nona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6dbc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_df_final = final_definitive_df_nona.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix_final = numeric_df_final.corr()\n",
    "\n",
    "correlation_matrix_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inf_viz_env)",
   "language": "python",
   "name": "inf_viz_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
